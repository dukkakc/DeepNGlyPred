{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSSM Feature extraction from individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/subash/SPIDER2_local/NGLYDE Positive and Negative spd3')\n",
    "def split_line(line):\n",
    "    Splitted_line = line.split()\n",
    "    return Splitted_line\n",
    "\n",
    "def open_file(file_name, sequence, position):\n",
    "    with open(file_name) as f_in:\n",
    "        window_size = len(sequence)\n",
    "        lines = f_in.readlines()[3:]\n",
    "        feature_vector = []\n",
    "        feature_vector.append(file_name[0:6])\n",
    "        feature_vector.append(position)\n",
    "        feature_vector.append(sequence)\n",
    "        \n",
    "        AA = []\n",
    "        alanine = []\n",
    "        arginine = []\n",
    "        asparagine = []\n",
    "        aspartic_acid = []\n",
    "        cysteine = []\n",
    "        glutamine = []\n",
    "        glutamic_acid = []\n",
    "        glycine = []\n",
    "        histidine = []\n",
    "        isoleucine = []\n",
    "        leucine = []\n",
    "        lysine = []\n",
    "        methionine = []\n",
    "        phenylalanine = []\n",
    "        proline = []\n",
    "        serine = []\n",
    "        threonine = []\n",
    "        tryptophan = []\n",
    "        tyrosine = []\n",
    "        valine = []\n",
    "        w_alanine = []\n",
    "        w_arginine = []\n",
    "        w_asparagine = []\n",
    "        w_aspartic_acid = []\n",
    "        w_cysteine = []\n",
    "        w_glutamine = []\n",
    "        w_glutamic_acid = []\n",
    "        w_glycine = []\n",
    "        w_histidine = []\n",
    "        w_isoleucine = []\n",
    "        w_leucine = []\n",
    "        w_lysine = []\n",
    "        w_methionine = []\n",
    "        w_phenylalanine = []\n",
    "        w_proline = []\n",
    "        w_serine = []\n",
    "        w_threonine = []\n",
    "        w_tryptophan = []\n",
    "        w_tyrosine = []\n",
    "        w_valine = []\n",
    "        for line in lines:\n",
    "            if line == '\\n':\n",
    "                break\n",
    "            else:\n",
    "                AA.append(split_line(line)[1])\n",
    "                alanine.append(split_line(line)[2])\n",
    "                arginine.append(split_line(line)[3])\n",
    "                asparagine.append(split_line(line)[4])\n",
    "                aspartic_acid.append(split_line(line)[5])\n",
    "                cysteine.append(split_line(line)[6])\n",
    "                glutamine.append(split_line(line)[7])\n",
    "                glutamic_acid.append(split_line(line)[8])\n",
    "                glycine.append(split_line(line)[9])\n",
    "                histidine.append(split_line(line)[10])\n",
    "                isoleucine.append(split_line(line)[11])\n",
    "                leucine.append(split_line(line)[12])\n",
    "                lysine.append(split_line(line)[13])\n",
    "                methionine.append(split_line(line)[14])\n",
    "                phenylalanine.append(split_line(line)[15])\n",
    "                proline.append(split_line(line)[16])\n",
    "                serine.append(split_line(line)[17])\n",
    "                threonine.append(split_line(line)[18])\n",
    "                tryptophan.append(split_line(line)[19])\n",
    "                tyrosine.append(split_line(line)[20])\n",
    "                valine.append(split_line(line)[21])\n",
    "\n",
    "                AMINO_ACID = ''.join([str(elem) for elem in AA])  \n",
    "    index = AMINO_ACID.find(sequence)\n",
    "    w_AA = AA[index:index+window_size]\n",
    "    w_alanine = alanine[index:index+window_size]\n",
    "    w_arginine = arginine[index:index+window_size]\n",
    "    w_asparagine = asparagine[index:index+window_size]\n",
    "    w_aspartic_acid = aspartic_acid[index:index+window_size]\n",
    "    w_cysteine = cysteine[index:index+window_size]\n",
    "    w_glutamine = glutamine[index:index+window_size]\n",
    "    w_glutamic_acid = glutamic_acid[index:index+window_size]\n",
    "    w_glycine = glycine[index:index+window_size]\n",
    "    w_histidine = histidine[index:index+window_size]\n",
    "    w_isoleucine = isoleucine[index:index+window_size]\n",
    "    w_leucine = leucine[index:index+window_size]\n",
    "    w_lysine = lysine[index:index+window_size]\n",
    "    w_methionine = methionine[index:index+window_size]\n",
    "    w_phenylalanine = phenylalanine[index:index+window_size]\n",
    "    w_proline = proline[index:index+window_size]\n",
    "    w_serine = serine[index:index+window_size]\n",
    "    w_threonine = threonine[index:index+window_size]\n",
    "    w_tryptophan = tryptophan[index:index+window_size]\n",
    "    w_tyrosine = tyrosine[index:index+window_size]\n",
    "    w_valine = valine[index:index+window_size]\n",
    "   \n",
    "    for i in range(len(w_AA)):\n",
    "        feature_vector.append(w_alanine[i])\n",
    "        feature_vector.append(w_arginine[i])\n",
    "        feature_vector.append(w_asparagine[i])\n",
    "        feature_vector.append(w_aspartic_acid[i])\n",
    "        feature_vector.append(w_cysteine[i])\n",
    "        feature_vector.append(w_glutamine[i])\n",
    "        feature_vector.append(w_glutamic_acid[i])\n",
    "        feature_vector.append(w_glycine[i])\n",
    "        feature_vector.append(w_histidine[i])\n",
    "        feature_vector.append(w_isoleucine[i])\n",
    "        feature_vector.append(w_leucine[i])\n",
    "        feature_vector.append(w_lysine[i])\n",
    "        feature_vector.append(w_methionine[i])\n",
    "        feature_vector.append(w_phenylalanine[i])\n",
    "        feature_vector.append(w_proline[i])\n",
    "        feature_vector.append(w_serine[i])\n",
    "        feature_vector.append(w_threonine[i])\n",
    "        feature_vector.append(w_tryptophan[i])\n",
    "        feature_vector.append(w_tyrosine[i])\n",
    "        feature_vector.append(w_valine[i])\n",
    "       \n",
    "    listToStr = ','.join([str(elem) for elem in feature_vector])\n",
    "       \n",
    "    with open('PSSM_April_24.csv','a+') as outfile:\n",
    "        outfile.write(listToStr)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "i = 0 \n",
    "with open('nglyde_positive_negative_do_again') as inputfile:\n",
    "    for line in inputfile:\n",
    "        x = line.split()\n",
    "        file_name = x[1]+\".pssm\"\n",
    "        print(file_name)\n",
    "        position = x[2]\n",
    "        sequence = x[4].strip(\"\\n\")\n",
    "        sequence = sequence.strip(\"X\")\n",
    "        i = i + 1\n",
    "        print(Sequence)\n",
    "        print(i)\n",
    "        open_file(file_name,sequence,position) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetSurfP - 2.0 Feature Extraction from Individual File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\philp\\Atlas netsurfp\")\n",
    "\n",
    "def Secondary_Structure(listn):\n",
    "    total = [\"C\", \"E\", \"H\"]  \n",
    "    mapping = {}  \n",
    "    for x in range(len(total)):\n",
    "        mapping[total[x]] = x\n",
    "    for c in listn:\n",
    "        arr = [0, 0, 0]\n",
    "        arr[mapping[c]] = 1      \n",
    "    return arr\n",
    "\n",
    "\n",
    "def open_file(pid,sequence,position):  \n",
    "    window_size = len(sequence)\n",
    "\n",
    "    with open(pid) as outfile:\n",
    "        feature_vector = []\n",
    "        AA = []\n",
    "        window_AA = []\n",
    "        rsa = []\n",
    "        window_rsa = []\n",
    "        asa = []\n",
    "        window_asa = []\n",
    "        SS = []\n",
    "        window_SS = []\n",
    "        pH = []\n",
    "        window_pH = []\n",
    "        pE = []\n",
    "        window_pE = []\n",
    "        pC = []\n",
    "        window_pC = []\n",
    "        phi = []\n",
    "        window_phi = []\n",
    "        psi = []\n",
    "        window_psi = []\n",
    "        disorder = []\n",
    "        window_disorder = []\n",
    "        lines = outfile.readlines()[1:]\n",
    "        for line in lines:\n",
    "            list = line.split(\",\")\n",
    "            AA.append(list[1])\n",
    "            rsa.append(list[3])\n",
    "            asa.append(list[4])\n",
    "            SS.append(list[5])\n",
    "            pH.append(list[6])\n",
    "            pE.append(list[7])\n",
    "            pC.append(list[8])\n",
    "            phi.append(list[18])\n",
    "            psi.append(list[19])\n",
    "            disorder.append(list[20])\n",
    "           \n",
    "        AA = ''.join(AA)\n",
    "        index = AA.find(sequence)\n",
    "       \n",
    "        window_AA = AA[index:index + window_size]\n",
    "        window_rsa = rsa [index:index+window_size]\n",
    "        window_asa = asa[index:index+window_size]\n",
    "        window_SS = SS[index:index+window_size]\n",
    "        window_pH = pH[index:index+window_size]\n",
    "        window_pE = pE[index:index+window_size]\n",
    "        window_pC = pC[index:index+window_size]\n",
    "        window_phi = phi[index:index+window_size]\n",
    "        window_psi = psi[index:index+window_size]\n",
    "        window_disorder = disorder[index:index+window_size]\n",
    "       \n",
    "        uniprot_accession_number = pid[0:6]\n",
    "        feature_vector.append(uniprot_accession_number)\n",
    "        feature_vector.append(position)\n",
    "        feature_vector.append(sequence)\n",
    "       \n",
    "        for i in range(len(window_AA)):\n",
    "            SS_list = Secondary_Structure(window_SS[i])\n",
    "            SS_string = \", \".join(repr(e) for e in SS_list)\n",
    "            feature_vector.append(window_rsa[i])\n",
    "            feature_vector.append(window_asa[i])\n",
    "            feature_vector.append(SS_string)\n",
    "#             feature_vector.append(window_pH[i])\n",
    "#             feature_vector.append(window_pE[i])\n",
    "#             feature_vector.append(window_pC[i])\n",
    "            feature_vector.append(window_phi[i])\n",
    "            feature_vector.append(window_psi[i])\n",
    "            window_disor = window_disorder[i].strip(\"\\n\")\n",
    "            feature_vector.append(window_disor)\n",
    "           \n",
    "        feat_Vec = ','.join([str(elem) for elem in feature_vector])\n",
    "        with open(\"51_WINDOW_INDEPENDENT.csv\",\"a+\") as Xiomi:\n",
    "            Xiomi.write(feat_Vec)\n",
    "            Xiomi.write(\"\\n\")\n",
    "       \n",
    "\n",
    "with open(\"51_WINDOW_INDEPENDENT.fasta\") as outputfile:\n",
    "    for line in outputfile:\n",
    "        x = line.split()\n",
    "        pid = x[0]+\".fasta.csv\"\n",
    "        sequence = x[2].strip(\"\\n\").strip(\"-\")\n",
    "        position = x[1]\n",
    "        open_file(pid,sequence,position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gapped Dipeptide Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diPep_pair(Pos_SeqWin_LS):\n",
    "    aaSeq=Pos_SeqWin_LS\n",
    "    down_LS=[]\n",
    "    up_LS=[]\n",
    "    downUP_LS=[]\n",
    "\n",
    "    for a, b in enumerate(aaSeq):\n",
    "        lenSeq=len(aaSeq)\n",
    "        centre=int((lenSeq)/2)\n",
    "        if aaSeq[centre] == \"N\" and (aaSeq[centre+2] == \"T\" or aaSeq[centre+2] == \"S\"):\n",
    "            if(a<centre):\n",
    "                differ=centre-(a+1)  \n",
    "                concat_down=b+\"-\"+str(differ)+aaSeq[centre]    \n",
    "                down_LS.append(concat_down)    \n",
    "            if (a>centre):\n",
    "                difference=a-(centre+1)          \n",
    "                concat_up=aaSeq[centre]+\"+\"+str(difference)+b  \n",
    "                up_LS.append(concat_up)      \n",
    "        down_LS.extend(up_LS)        \n",
    "        up_LS=[]\n",
    "    return(down_LS)\n",
    "\n",
    "with open(\"nglyde_positive_negative_do_again\") as FFF:\n",
    "    for line in FFF:\n",
    "        # April_24.txt is the further file used for further processing\n",
    "        with open(\"April_24.txt\",\"a+\") as GGG:\n",
    "            x = line.split()\n",
    "            GGG.write(x[0])\n",
    "            GGG.write(\"\\t\")\n",
    "            GGG.write(x[1])\n",
    "            GGG.write(\"\\t\")\n",
    "            GGG.write(x[2])\n",
    "            GGG.write(\"\\t\")\n",
    "            GGG.write(x[3])\n",
    "            GGG.write(\"\\t\")\n",
    "            GGG.write(x[4])\n",
    "            GGG.write(\"\\t\")\n",
    "            sss = x[4].strip(\"\\n\")\n",
    "            GHT = diPep_pair(sss)\n",
    "            listToStr = ','.join([str(elem) for elem in GHT])\n",
    "            GGG.write(listToStr)\n",
    "            GGG.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Odds ratio calculated by the percentage of its occurances in glycosites divided by the percentage of its occurance in non-glycosite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getList_key(dict):      \n",
    "    return list(dict.keys())\n",
    "\n",
    "def getList_value(dict):\n",
    "    return list(dict.values())\n",
    "   \n",
    "\n",
    "def min_max(my_dict):\n",
    "    key_max = max(my_dict.keys(), key=(lambda k: my_dict[k]))\n",
    "    key_min = min(my_dict.keys(), key=(lambda k: my_dict[k]))\n",
    "   \n",
    "    max_value = my_dict[key_max]\n",
    "    min_value = my_dict[key_min]\n",
    "   \n",
    "    print('Maximum Value: ',max_value)\n",
    "    print('Minimum Value: ',min_value)\n",
    "   \n",
    "    for i in my_dict:\n",
    "        my_dict[i] = float(my_dict[i]-min_value/max_value-min_value)\n",
    "       \n",
    "    print(my_dict)\n",
    "\n",
    "def div_d(my_dict,my_dict_1):\n",
    "    sum_p = sum(my_dict.values())\n",
    "    sum_n = sum(my_dict_1.values())\n",
    "   \n",
    "    for i in my_dict:\n",
    "        my_dict[i] = float(my_dict[i]/sum_p)\n",
    "       \n",
    "    for i in my_dict_1:\n",
    "        my_dict_1[i] = float(my_dict_1[i]/sum_n)\n",
    "       \n",
    "    new_dict = {}\n",
    "    for key in my_dict:\n",
    "        if key in my_dict_1:\n",
    "            new_dict[key]=my_dict[key]/my_dict_1[key]\n",
    "        else:\n",
    "            new_dict[key] = 1\n",
    "           \n",
    "    for key in my_dict_1:\n",
    "        if key not in my_dict:\n",
    "            new_dict[key] = 1\n",
    "           \n",
    "    print(\"The odds ratio calculated by the percentage of its occurrences\\\n",
    "           in glycosites divided by the percentage of its occurrences in \\\n",
    "           non-glycosites: \",new_dict)\n",
    "    \n",
    "    print(\"\\n\\n\\n\")\n",
    "    q = getList_key(new_dict)\n",
    "    r = getList_value(new_dict)\n",
    "    print(q)\n",
    "    print(r)\n",
    "    print(len(q))\n",
    "    print(len(r))\n",
    "    for i in range(len(q)):\n",
    "        dr = str(q[i].strip(''))\n",
    "        cr = str(r[i])\n",
    "        print(dr,\"\\t\",cr)\n",
    "        # pos_vs_neg_data_mer_25.txt is the required file for further processing\n",
    "        with open(\"pos_vs_neg_data_mer_25.txt\",\"a+\") as output:\n",
    "            output.write(dr)\n",
    "            output.write(\"\\t\")\n",
    "            output.write(cr)\n",
    "            output.write(\"\\n\")\n",
    "   \n",
    "\n",
    "def CountFrequency(my_list_positive, my_list_negative):  \n",
    "    freq = {}\n",
    "    freq_1 = {}\n",
    "    for item in my_list_positive:\n",
    "        if (item in freq):\n",
    "            freq[item] += 1\n",
    "        else:\n",
    "            freq[item] = 1\n",
    "           \n",
    "    for item in my_list_negative:\n",
    "        if (item in freq_1):\n",
    "            freq_1[item] += 1\n",
    "        else:\n",
    "            freq_1[item] = 1\n",
    " \n",
    "    for key, value in freq.items():\n",
    "        print ((key, value))\n",
    "       \n",
    "    print(\"\\n\\n\\n\")            \n",
    "\n",
    "    div_d(freq,freq_1)\n",
    "   \n",
    "\n",
    "with open(\"April_24.txt\", \"r\") as openfile:\n",
    "    big_list_positive = []\n",
    "    big_list_negative = []\n",
    "    line = openfile.readlines()\n",
    "    for line in line:\n",
    "        x = line.split()\n",
    "        a = list(x[4].split(\",\"))\n",
    "        if x[0] == \"1\":\n",
    "            for element in a:\n",
    "                big_list_positive.append(element)\n",
    "        if x[0] == \"0\":\n",
    "            for element in a:\n",
    "                big_list_negative.append(element)\n",
    "           \n",
    "    CountFrequency(big_list_positive,big_list_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## saving GDP ratio as dictionary with GDP as Key and ratio values as Values\n",
    "def GDP_ratio(inputFile):  \n",
    "       \n",
    "    gdp_LS=[]\n",
    "    ratio_LS=[]\n",
    "    gdpRatio_DI={}\n",
    "       \n",
    "    for ratio in open_GDPratio.readlines()[1:]:\n",
    "\n",
    "        ratioSplit=re.split(r'\\t', ratio.replace('\\n', ''))\n",
    "\n",
    "        gdp=ratioSplit[0]\n",
    "        ratio=ratioSplit[1]\n",
    "           \n",
    "        gdp_LS.append(gdp)\n",
    "        ratio_LS.append(ratio)\n",
    "           \n",
    "        gdpRatio_DI=dict(zip(gdp_LS, ratio_LS))\n",
    "       \n",
    "    return(gdpRatio_DI)\n",
    "           \n",
    "\n",
    "## Looping through each gapped dipeptide and retriving their information from dictionary and saving them in list format            \n",
    "       \n",
    "def GDP_sw(seqWin, DI_GDPratio):    \n",
    "   \n",
    "    gdpRatioSW_LS = []\n",
    "    gdpSeqWin_LS =[]\n",
    "   \n",
    "    for GDP_seqWin in seqWin.readlines()[1:] :\n",
    "        gdpSeqWin_LS.append(GDP_seqWin.replace('\\n', ''))\n",
    "       \n",
    "        split_GDP = re.split(r'\\t', GDP_seqWin.replace('\\n', ''))        \n",
    "       \n",
    "        gdp_pair=re.split(r',', split_GDP[5])\n",
    "        #print(gdp_pair)\n",
    "       \n",
    "   \n",
    "        mapRatio_GDP_LS=[]\n",
    "   \n",
    "        for each_GDP in gdp_pair:\n",
    "            mapValue = DI_GDPratio.get(each_GDP)\n",
    "            mapRatio_GDP_LS.append(mapValue)\n",
    "        #print(mapRatio_GDP_LS)\n",
    "        gdpRatioSW_LS.append(mapRatio_GDP_LS)\n",
    "    #print(gdpRatioSW_LS)\n",
    "    #print(len(gdpRatioSW_LS))\n",
    "    return(gdpRatioSW_LS, gdpSeqWin_LS)\n",
    "\n",
    "\n",
    "def ratioNormalize(ratio_GDP, gdpInfo_LS, outFile):\n",
    "   \n",
    "    #print(len(ratio_GDP))\n",
    "   \n",
    "    ## Converting the rows data to column data\n",
    "    actual_Ratio=np.array(ratio_GDP)\n",
    "    print(\"# of elements in the array from actual data ::\" +str(actual_Ratio.size))\n",
    "    print(\"# of Rows and # of Columns from actual data ::\" +str(actual_Ratio.shape))\n",
    "\n",
    "    tranpose_Ratio=actual_Ratio.T\n",
    "    print(\"# of elements in the array from Transposed data ::\" +str(tranpose_Ratio.size))\n",
    "    print(\"# of Rows and # of Columns from Transposed data ::\" +str(tranpose_Ratio.shape))\n",
    "\n",
    "    print(tranpose_Ratio)\n",
    "\n",
    "    ## Taking the array information and Looong through array  finding Minimum and Maximum values to normalize the GDP values\n",
    "\n",
    "    normalizeVal_TransposeArray_LS=[]\n",
    "\n",
    "    for eachRow in tranpose_Ratio:\n",
    "        #print(eachRow)\n",
    "   \n",
    "        ## Converting the elements in List from string format to Float to find them Minimum and maximum values\n",
    "\n",
    "        eachRow_LS=[]\n",
    "       \n",
    "\n",
    "        for b in eachRow:\n",
    "            #print(b)\n",
    "            #new_b=float(b)\n",
    "            #new_b = b.replace('None', 0.0)\n",
    "            eachRow_LS.append(float(b))\n",
    "            #print(eachRow_LS)\n",
    "       \n",
    "        min_RowVal=min((eachRow_LS))\n",
    "        #print('Minimum value from Transposed Array for each Position ::' +str(min_RowVal))\n",
    "\n",
    "        subtract_Ratio_LS=[]\n",
    "   \n",
    "        for eachPosition in eachRow:\n",
    "            #print(len(eachPosition))\n",
    "            subtract_Ratio=float(eachPosition)-float(min_RowVal)            ## for input where the encoded GDP values are the ratios\n",
    "\n",
    "            #print(eachPosition , min_RowVal ,subtract_Ratio)\n",
    "            subtract_Ratio_LS.append(subtract_Ratio)\n",
    "            #print(subtract_Ratio_LS)\n",
    "            max_RowVal=max(subtract_Ratio_LS)\n",
    "            #print('Maximum value from Transposed Array for each Position after minus with MinimumValue ::' +str(max_RowVal))\n",
    "\n",
    "\n",
    "        normalizeValue_LS=[]\n",
    "\n",
    "        for eachSubPosition in subtract_Ratio_LS:\n",
    "            if eachSubPosition !=0.0:\n",
    "                divide_Ratio=eachSubPosition/max_RowVal\n",
    "                #print(eachSubPosition   , max_RowVal   , divide_Ratio)\n",
    "                normalizeValue_LS.append(format(divide_Ratio, '.4f'))           ## Rounding value to 4 decimal places\n",
    "            else:\n",
    "                divide_Ratio=0.0000\n",
    "                normalizeValue_LS.append(divide_Ratio)\n",
    "\n",
    "        ## Normalized Values List is again appended to another List inorder to make data in Array Format required to transpose back to Actual array\n",
    "\n",
    "        normalizeVal_TransposeArray_LS.append(normalizeValue_LS)\n",
    "\n",
    "    Transposed_NormalizeVal=np.array(normalizeVal_TransposeArray_LS)\n",
    "    print(\"# of elements in the Normalized array from Transposed data ::\" +str(Transposed_NormalizeVal.size))\n",
    "    print(\"# of Rows and # of Columns in Normalized array from Transposed data ::\" +str(Transposed_NormalizeVal.shape))\n",
    "\n",
    "    Actual_NormalizeVal=Transposed_NormalizeVal.T\n",
    "    print(\"# of elements in the Normalized array from actual data ::\" +str(Actual_NormalizeVal.size))\n",
    "    print(\"# of Rows and # of Columns in Normalized array from actual data ::\" +str(Actual_NormalizeVal.shape))\n",
    "\n",
    "    ## writing output to file\n",
    "   \n",
    "    for eachGDP_ref, mapValue in zip(gdpInfo_LS,  Actual_NormalizeVal):\n",
    "        mapNormalizeVal=eachGDP_ref+'\\t'+','.join(str(eachVal) for eachVal in mapValue)+'\\n'\n",
    "        outFile.write(mapNormalizeVal)  \n",
    "\n",
    "## Input Files (Gapped Dipeptides)\n",
    "\n",
    "inGDP_seqWinPath = '/home/subash/Gapped Dipeptide Features/Gapped_pairWisePattern/'\n",
    "\n",
    "inGDP_RatioPath = '/home/subash/Gapped Dipeptide Features/Positive_Ratio_PairWisePattern/'\n",
    "\n",
    "## Output Files\n",
    "outFile_path = '/home/subash/Gapped Dipeptide Features/NormalizedGDP/'\n",
    "\n",
    "  \n",
    "gdp_fileName = 'April_24.txt'\n",
    "inFile_GDP = inGDP_seqWinPath+gdp_fileName\n",
    "print(inFile_GDP)\n",
    "\n",
    "ratio_fileName = 'pos_vs_neg_data_mer_25.txt'\n",
    "inFile_GDPratio = inGDP_RatioPath+ratio_fileName\n",
    "print(inFile_GDPratio)\n",
    "\n",
    "a = 25 \n",
    "\n",
    "outFileName = '3080mot_Train_NlinkGP_GapPairs_'+str(a)+'sw_Ratio_Normalized.txt'\n",
    "outputFile = outFile_path + outFileName\n",
    "print(outputFile)\n",
    "\n",
    "seqWindow = str(a)+'-merSeqWindow'\n",
    "GappedDiPeptides = str(a-1)+'-GappedDiPeptides'\n",
    "       \n",
    "## Opening input files\n",
    "open_GDPratio = open(inFile_GDPratio, 'r')\n",
    "\n",
    "open_GDPseqWin = open(inFile_GDP, 'r')\n",
    "\n",
    "## Opening output files\n",
    "open_outputFile = open(outputFile, 'w')\n",
    "\n",
    "open_outputFile.write('UniProt_Evidence'+'\\t'+'ID'+'\\t'+'NsequonPos'+'\\t'+'Nsequon'+'\\t'+seqWindow+'\\t'+GappedDiPeptides+'\\t'+'GDP_RatioValue_Normalized'+'\\n')\n",
    "\n",
    "\n",
    "## Calling functions\n",
    "\n",
    "#readGDP(inGDP_seqWinPath, inGDP_RatioPath)\n",
    "ratioGDP = GDP_ratio(open_GDPratio)\n",
    "#print(len(ratioGDP))\n",
    "\n",
    "seqWinGDP_ratio = GDP_sw(open_GDPseqWin, ratioGDP)\n",
    "\n",
    "ratioNormalize(seqWinGDP_ratio[0], seqWinGDP_ratio[1], open_outputFile)\n",
    "\n",
    "## Close files\n",
    "open_GDPratio.close()\n",
    "open_GDPseqWin.close()\n",
    "open_outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the Training File into Fasta Formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file = open(\"NGLYDE_FASTA_formate.fasta\",'a+')\n",
    "\n",
    "with open(\"nglyde_positive_negative_do_again\") as PhD:\n",
    "    for line in PhD:\n",
    "        line = line.split()\n",
    "        a = line[1]\n",
    "        b = line[2]\n",
    "        c = line[4]\n",
    "        write_to_file.write(\">\")\n",
    "        write_to_file.write(a)\n",
    "        write_to_file.write(\"|\")\n",
    "        write_to_file.write(b)\n",
    "        write_to_file.write(\"\\n\")\n",
    "        write_to_file.write(c)\n",
    "        write_to_file.write(\"\\n\")\n",
    "        \n",
    "write_to_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the data into Facebook Protein Encoding Formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "data = []\n",
    "for seq_record in SeqIO.parse(\"NGLYDE_FASTA_formate.fasta\",\"fasta\"):\n",
    "    placeholder = seq_record.id\n",
    "    Sequence = str(seq_record.seq)\n",
    "    inner_list_containing_two_element = []\n",
    "    first_protein_id = str(\">\"+placeholder)\n",
    "    inner_list_containing_two_element.append(first_protein_id)\n",
    "    inner_list_containing_two_element.append(Sequence)\n",
    "    convert_to_tuple = tuple(inner_list_containing_two_element)\n",
    "    data.append(convert_to_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook Protein Encoding Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user fair-esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import esm\n",
    "import tqdm\n",
    "import torch\n",
    "model, alphabet = torch.hub.load(\"facebookresearch/esm\", \"esm1b_t33_650M_UR50S\")\n",
    "\n",
    "# Load ESM-1b model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "# Prepare data (first 2 sequences from ESMStructuralSplitDataset superfamily / 4)\n",
    "\n",
    "batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "# Extract per-residue representations (on CPU)\n",
    "with torch.no_grad():\n",
    "    results = model(batch_tokens, repr_layers=[33], return_contacts=True)\n",
    "token_representations = results[\"representations\"][33]\n",
    "\n",
    "# Generate per-sequence representations via averaging\n",
    "# NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "sequence_representations = []\n",
    "for i, (_, seq) in tqdm.tqdm(enumerate(data)):\n",
    "    sequence_representations.append(token_representations[i, 1 : len(seq) + 1].mean(0))\n",
    "\n",
    "\n",
    "d = np.stack(sequence_representations,axis=0)\n",
    "dataframe = pd.DataFrame(d)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv('facebook_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prot_bert Protein Embedding Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False ) \n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "motif = []\n",
    "tensor_to_be_trained = []\n",
    "\n",
    "with open(\"nglyde_positive_negative_do_again\") as Daughter:\n",
    "    for line in Daughter:\n",
    "        line = line.split()\n",
    "        line = line[4]\n",
    "        line = line.strip(\"\\n\")\n",
    "        line = line.replace(\"-\",\"X\")\n",
    "        line = \" \".join(line)\n",
    "        motif.append(line)\n",
    "            \n",
    "def feature_extractor(sequence_Example):\n",
    "    sequence_Example = re.sub(r\"[UZOB]\", \"X\", sequence_Example)\n",
    "    encoded_input = tokenizer(sequence_Example, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    nilam_to_us = output[0][:,0,:].detach().numpy()\n",
    "    tensor_to_be_trained.append(nilam_to_us)     \n",
    "    \n",
    "size_length = len(motif)\n",
    "tensor_list = []\n",
    "for i in tqdm.tqdm(range(size_length)):\n",
    "    feature_extractor(motif[i])\n",
    "\n",
    "positive = np.vstack(tensor_to_be_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the array into DataFrame to sa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
