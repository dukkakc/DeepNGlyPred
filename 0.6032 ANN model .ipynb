{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "h9kcAp5k0QI4",
    "outputId": "05de926c-5583-4c52-98a5-99746f32dbb9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report,auc\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dense, Bidirectional,Dense, LSTM, Activation, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"All_feature_Independent_dataframe.csv\")\n",
    "\n",
    "X_test_NetSurfP2 = df_test.iloc[:,2100:2428]\n",
    "\n",
    "y_test_NetSurfP2 = df_test[\"Target\"]\n",
    "\n",
    "\n",
    "X_test_NetSurfP2.fillna(0,inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_NetSurfP2 = scaler.fit_transform(X_test_NetSurfP2)\n",
    "\n",
    "\n",
    "X_test_NetSurfP2 = np.array(X_test_NetSurfP2)\n",
    "y_test_NetSurfP2 = np.array(y_test_NetSurfP2)\n",
    "\n",
    "X_test_PSSM_test = df_test.iloc[:,1280:2100]\n",
    "\n",
    "y_test_PSSM = df_test[\"Target\"]\n",
    "\n",
    "X_test_PSSM_test.fillna(0,inplace=True)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_PSSM_test = scaler.fit_transform(X_test_PSSM_test)\n",
    "\n",
    "\n",
    "X_test_PSSM_test = np.array(X_test_PSSM_test)\n",
    "y_test_PSSM = np.array(y_test_PSSM)\n",
    "\n",
    "X_test_Gapped_Dipeptide = df_test.iloc[:,2428:2468]\n",
    "\n",
    "y_test_Gapped_Dipeptide = df_test[\"Target\"]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_Gapped_Dipeptide = scaler.fit_transform(X_test_Gapped_Dipeptide)\n",
    "\n",
    "\n",
    "X_test_Gapped_Dipeptide = np.array(X_test_Gapped_Dipeptide)\n",
    "y_test_Gapped_Dipeptide = np.array(y_test_Gapped_Dipeptide)\n",
    "\n",
    "X_test_PSSM_NetSurfP2_Gapped_Dipeptide = np.hstack((X_test_PSSM_test,X_test_NetSurfP2,X_test_Gapped_Dipeptide))\n",
    "\n",
    "y_test_PSSM_NetSurfP2_Gapped_Dipeptide = y_test_Gapped_Dipeptide\n",
    "\n",
    "X_test = X_test_PSSM_NetSurfP2_Gapped_Dipeptide\n",
    "\n",
    "y_test = y_test_PSSM_NetSurfP2_Gapped_Dipeptide\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"Glycosylation_model__26__.h5\")\n",
    "Y_pred = model.predict(X_test)\n",
    "DNN_prediction_list = []\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = (Y_pred > 0.5)\n",
    "fp = open(\"DNN_prediction.txt\",\"a+\")\n",
    "result_obtained = []\n",
    "y_pred = [np.argmax(y, axis=None, out=None) for y in Y_pred]\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        fp.write(\"yes\")\n",
    "        fp.write(\"\\n\")\n",
    "        DNN_prediction_list.append(\"yes\")\n",
    "    else:\n",
    "        fp.write(\"no\")\n",
    "        fp.write(\"\\n\")\n",
    "        DNN_prediction_list.append(\"no\")\n",
    "\n",
    "fp.close()\n",
    "# y_pred = np.array(y_pred)\n",
    "# print(\"Matthews Correlation : \",matthews_corrcoef(y_test, y_pred))\n",
    "# print(\"\\n\")\n",
    "# print(\"Confusion Matrix : \\n\\n\",confusion_matrix(y_test, y_pred))\n",
    "# print(\"\\n\")\n",
    "# print(\"Accuracy on test set:   \",accuracy_score(y_test, y_pred))\n",
    "# print(\"\\n\")\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "TP = cm[1][1]\n",
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "\n",
    "# mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Sensitivity = TP/(TP+FN)\n",
    "\n",
    "# Specificity = TN/(TN+FP)\n",
    "\n",
    "# Precision = TP/(TP+FP)\n",
    "# print(\"Sensitivity:   \",Sensitivity,\"\\t\",\"Specificity:   \",Specificity)\n",
    "# print(\"\\n\")\n",
    "# print(\"Precision:   \",Precision)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "lfA-PCPQ0QI-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[207,  73],\n",
       "       [ 19, 148]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DNN_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680\n",
      "0\n",
      "4200\n",
      "0\n",
      "(447, 1188) (447,)\n",
      "0 yes\n",
      "1 yes\n",
      "2 yes\n",
      "3 no\n",
      "4 no\n",
      "5 no\n",
      "6 yes\n",
      "7 yes\n",
      "8 yes\n",
      "9 yes\n",
      "10 yes\n",
      "11 yes\n",
      "12 yes\n",
      "13 yes\n",
      "14 yes\n",
      "15 no\n",
      "16 yes\n",
      "17 yes\n",
      "18 no\n",
      "19 yes\n",
      "20 yes\n",
      "21 yes\n",
      "22 yes\n",
      "23 no\n",
      "24 yes\n",
      "25 yes\n",
      "26 yes\n",
      "27 yes\n",
      "28 yes\n",
      "29 yes\n",
      "30 no\n",
      "31 yes\n",
      "32 yes\n",
      "33 yes\n",
      "34 no\n",
      "35 no\n",
      "36 no\n",
      "37 yes\n",
      "38 yes\n",
      "39 yes\n",
      "40 yes\n",
      "41 no\n",
      "42 yes\n",
      "43 yes\n",
      "44 yes\n",
      "45 yes\n",
      "46 no\n",
      "47 yes\n",
      "48 yes\n",
      "49 yes\n",
      "50 yes\n",
      "51 yes\n",
      "52 yes\n",
      "53 yes\n",
      "54 yes\n",
      "55 yes\n",
      "56 yes\n",
      "57 yes\n",
      "58 yes\n",
      "59 yes\n",
      "60 yes\n",
      "61 yes\n",
      "62 yes\n",
      "63 yes\n",
      "64 yes\n",
      "65 yes\n",
      "66 yes\n",
      "67 yes\n",
      "68 yes\n",
      "69 yes\n",
      "70 yes\n",
      "71 yes\n",
      "72 yes\n",
      "73 yes\n",
      "74 no\n",
      "75 no\n",
      "76 yes\n",
      "77 yes\n",
      "78 yes\n",
      "79 no\n",
      "80 yes\n",
      "81 yes\n",
      "82 yes\n",
      "83 yes\n",
      "84 no\n",
      "85 yes\n",
      "86 yes\n",
      "87 yes\n",
      "88 yes\n",
      "89 yes\n",
      "90 no\n",
      "91 yes\n",
      "92 yes\n",
      "93 yes\n",
      "94 yes\n",
      "95 yes\n",
      "96 yes\n",
      "97 yes\n",
      "98 yes\n",
      "99 yes\n",
      "100 yes\n",
      "101 yes\n",
      "102 yes\n",
      "103 yes\n",
      "104 yes\n",
      "105 yes\n",
      "106 yes\n",
      "107 yes\n",
      "108 yes\n",
      "109 yes\n",
      "110 no\n",
      "111 yes\n",
      "112 yes\n",
      "113 no\n",
      "114 yes\n",
      "115 yes\n",
      "116 yes\n",
      "117 yes\n",
      "118 yes\n",
      "119 yes\n",
      "120 yes\n",
      "121 yes\n",
      "122 yes\n",
      "123 yes\n",
      "124 yes\n",
      "125 yes\n",
      "126 yes\n",
      "127 yes\n",
      "128 yes\n",
      "129 no\n",
      "130 no\n",
      "131 yes\n",
      "132 yes\n",
      "133 no\n",
      "134 yes\n",
      "135 yes\n",
      "136 yes\n",
      "137 yes\n",
      "138 yes\n",
      "139 yes\n",
      "140 yes\n",
      "141 yes\n",
      "142 yes\n",
      "143 yes\n",
      "144 yes\n",
      "145 yes\n",
      "146 yes\n",
      "147 yes\n",
      "148 yes\n",
      "149 yes\n",
      "150 yes\n",
      "151 yes\n",
      "152 yes\n",
      "153 yes\n",
      "154 yes\n",
      "155 yes\n",
      "156 no\n",
      "157 yes\n",
      "158 yes\n",
      "159 yes\n",
      "160 no\n",
      "161 yes\n",
      "162 yes\n",
      "163 yes\n",
      "164 yes\n",
      "165 yes\n",
      "166 yes\n",
      "167 yes\n",
      "168 yes\n",
      "169 no\n",
      "170 no\n",
      "171 yes\n",
      "172 no\n",
      "173 yes\n",
      "174 no\n",
      "175 no\n",
      "176 yes\n",
      "177 no\n",
      "178 no\n",
      "179 no\n",
      "180 no\n",
      "181 no\n",
      "182 yes\n",
      "183 no\n",
      "184 no\n",
      "185 no\n",
      "186 no\n",
      "187 no\n",
      "188 yes\n",
      "189 no\n",
      "190 yes\n",
      "191 yes\n",
      "192 no\n",
      "193 yes\n",
      "194 no\n",
      "195 no\n",
      "196 no\n",
      "197 yes\n",
      "198 no\n",
      "199 yes\n",
      "200 no\n",
      "201 no\n",
      "202 yes\n",
      "203 no\n",
      "204 yes\n",
      "205 yes\n",
      "206 yes\n",
      "207 no\n",
      "208 yes\n",
      "209 yes\n",
      "210 yes\n",
      "211 yes\n",
      "212 yes\n",
      "213 yes\n",
      "214 yes\n",
      "215 yes\n",
      "216 yes\n",
      "217 no\n",
      "218 yes\n",
      "219 no\n",
      "220 yes\n",
      "221 yes\n",
      "222 no\n",
      "223 no\n",
      "224 no\n",
      "225 no\n",
      "226 no\n",
      "227 no\n",
      "228 no\n",
      "229 no\n",
      "230 no\n",
      "231 no\n",
      "232 no\n",
      "233 no\n",
      "234 no\n",
      "235 yes\n",
      "236 no\n",
      "237 no\n",
      "238 no\n",
      "239 no\n",
      "240 no\n",
      "241 yes\n",
      "242 yes\n",
      "243 no\n",
      "244 no\n",
      "245 yes\n",
      "246 no\n",
      "247 no\n",
      "248 no\n",
      "249 no\n",
      "250 no\n",
      "251 no\n",
      "252 no\n",
      "253 no\n",
      "254 no\n",
      "255 no\n",
      "256 no\n",
      "257 yes\n",
      "258 no\n",
      "259 no\n",
      "260 no\n",
      "261 yes\n",
      "262 no\n",
      "263 no\n",
      "264 no\n",
      "265 no\n",
      "266 no\n",
      "267 no\n",
      "268 no\n",
      "269 no\n",
      "270 no\n",
      "271 no\n",
      "272 no\n",
      "273 no\n",
      "274 no\n",
      "275 no\n",
      "276 no\n",
      "277 no\n",
      "278 no\n",
      "279 no\n",
      "280 no\n",
      "281 no\n",
      "282 no\n",
      "283 no\n",
      "284 no\n",
      "285 yes\n",
      "286 no\n",
      "287 no\n",
      "288 yes\n",
      "289 yes\n",
      "290 yes\n",
      "291 yes\n",
      "292 no\n",
      "293 yes\n",
      "294 yes\n",
      "295 yes\n",
      "296 no\n",
      "297 no\n",
      "298 no\n",
      "299 no\n",
      "300 no\n",
      "301 no\n",
      "302 yes\n",
      "303 yes\n",
      "304 no\n",
      "305 no\n",
      "306 no\n",
      "307 no\n",
      "308 no\n",
      "309 no\n",
      "310 no\n",
      "311 no\n",
      "312 no\n",
      "313 no\n",
      "314 no\n",
      "315 no\n",
      "316 no\n",
      "317 no\n",
      "318 no\n",
      "319 no\n",
      "320 no\n",
      "321 no\n",
      "322 no\n",
      "323 no\n",
      "324 no\n",
      "325 yes\n",
      "326 no\n",
      "327 no\n",
      "328 no\n",
      "329 yes\n",
      "330 no\n",
      "331 no\n",
      "332 no\n",
      "333 no\n",
      "334 no\n",
      "335 no\n",
      "336 no\n",
      "337 no\n",
      "338 no\n",
      "339 no\n",
      "340 no\n",
      "341 no\n",
      "342 no\n",
      "343 no\n",
      "344 no\n",
      "345 no\n",
      "346 no\n",
      "347 no\n",
      "348 no\n",
      "349 no\n",
      "350 no\n",
      "351 no\n",
      "352 no\n",
      "353 no\n",
      "354 no\n",
      "355 no\n",
      "356 no\n",
      "357 no\n",
      "358 no\n",
      "359 no\n",
      "360 no\n",
      "361 yes\n",
      "362 no\n",
      "363 no\n",
      "364 no\n",
      "365 no\n",
      "366 yes\n",
      "367 no\n",
      "368 yes\n",
      "369 no\n",
      "370 no\n",
      "371 yes\n",
      "372 yes\n",
      "373 yes\n",
      "374 no\n",
      "375 no\n",
      "376 no\n",
      "377 no\n",
      "378 no\n",
      "379 no\n",
      "380 no\n",
      "381 yes\n",
      "382 yes\n",
      "383 yes\n",
      "384 no\n",
      "385 no\n",
      "386 yes\n",
      "387 no\n",
      "388 no\n",
      "389 no\n",
      "390 no\n",
      "391 no\n",
      "392 no\n",
      "393 no\n",
      "394 no\n",
      "395 no\n",
      "396 no\n",
      "397 yes\n",
      "398 yes\n",
      "399 no\n",
      "400 no\n",
      "401 no\n",
      "402 no\n",
      "403 no\n",
      "404 no\n",
      "405 no\n",
      "406 no\n",
      "407 yes\n",
      "408 no\n",
      "409 no\n",
      "410 no\n",
      "411 no\n",
      "412 no\n",
      "413 no\n",
      "414 no\n",
      "415 no\n",
      "416 yes\n",
      "417 no\n",
      "418 no\n",
      "419 no\n",
      "420 no\n",
      "421 no\n",
      "422 no\n",
      "423 no\n",
      "424 no\n",
      "425 no\n",
      "426 yes\n",
      "427 yes\n",
      "428 yes\n",
      "429 yes\n",
      "430 yes\n",
      "431 yes\n",
      "432 yes\n",
      "433 yes\n",
      "434 yes\n",
      "435 yes\n",
      "436 no\n",
      "437 yes\n",
      "438 no\n",
      "439 yes\n",
      "440 no\n",
      "441 no\n",
      "442 yes\n",
      "443 no\n",
      "444 no\n",
      "445 yes\n",
      "446 no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:313: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report,auc\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dense, Bidirectional,Dense, LSTM, Activation, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "import os \n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"All_feature_Independent_dataframe.csv\")\n",
    "\n",
    "X_test_NetSurfP2 = df_test.iloc[:,2100:2428]\n",
    "\n",
    "y_test_NetSurfP2 = df_test[\"Target\"]\n",
    "\n",
    "\n",
    "print(X_test_NetSurfP2.isnull().sum().sum())\n",
    "X_test_NetSurfP2.fillna(0,inplace=True)\n",
    "print(X_test_NetSurfP2.isnull().sum().sum())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_NetSurfP2 = scaler.fit_transform(X_test_NetSurfP2)\n",
    "\n",
    "\n",
    "X_test_NetSurfP2 = np.array(X_test_NetSurfP2)\n",
    "y_test_NetSurfP2 = np.array(y_test_NetSurfP2)\n",
    "\n",
    "X_test_PSSM_test = df_test.iloc[:,1280:2100]\n",
    "\n",
    "y_test_PSSM = df_test[\"Target\"]\n",
    "\n",
    "print(X_test_PSSM_test.isnull().sum().sum())\n",
    "X_test_PSSM_test.fillna(0,inplace=True)\n",
    "print(X_test_PSSM_test.isnull().sum().sum())\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_PSSM_test = scaler.fit_transform(X_test_PSSM_test)\n",
    "\n",
    "\n",
    "X_test_PSSM_test = np.array(X_test_PSSM_test)\n",
    "y_test_PSSM = np.array(y_test_PSSM)\n",
    "\n",
    "X_test_Gapped_Dipeptide = df_test.iloc[:,2428:2468]\n",
    "\n",
    "y_test_Gapped_Dipeptide = df_test[\"Target\"]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "X_test_Gapped_Dipeptide = scaler.fit_transform(X_test_Gapped_Dipeptide)\n",
    "\n",
    "\n",
    "X_test_Gapped_Dipeptide = np.array(X_test_Gapped_Dipeptide)\n",
    "y_test_Gapped_Dipeptide = np.array(y_test_Gapped_Dipeptide)\n",
    "\n",
    "X_test_PSSM_NetSurfP2_Gapped_Dipeptide = np.hstack((X_test_PSSM_test,X_test_NetSurfP2,X_test_Gapped_Dipeptide))\n",
    "\n",
    "y_test_PSSM_NetSurfP2_Gapped_Dipeptide = y_test_Gapped_Dipeptide\n",
    "\n",
    "X_test = X_test_PSSM_NetSurfP2_Gapped_Dipeptide\n",
    "\n",
    "y_test = y_test_PSSM_NetSurfP2_Gapped_Dipeptide\n",
    "\n",
    "print(X_test.shape,y_test.shape)\n",
    "svm_prediction_list = []\n",
    "\n",
    "fp = open(\"log_reg_prediction.txt\",\"a+\")\n",
    "# save the model to the disk \n",
    "filename_log_reg ='logistic_regression_model_nucleus_mitochondrion.sav'\n",
    "\n",
    "# some time later ...\n",
    "\n",
    "# load the model from disk \n",
    "model_log_reg = pickle.load(open(filename_log_reg,'rb'))\n",
    "\n",
    "y_pred = model_log_reg.predict(X_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        print(i, \"yes\")\n",
    "        fp.write(\"yes\")\n",
    "        fp.write(\"\\n\")\n",
    "        svm_prediction_list.append(\"yes\")\n",
    "    else:\n",
    "        print(i, \"no\")\n",
    "        fp.write(\"no\")\n",
    "        fp.write(\"\\n\")\n",
    "        svm_prediction_list.append(\"no\")\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(svm_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DNN_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_Yes = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and svm_prediction_list[i] == \"yes\":\n",
    "        Yes_Yes.append(\"Yes_Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_No = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and svm_prediction_list[i] == \"no\":\n",
    "        Yes_No.append(\"Yes_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_yes = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and svm_prediction_list[i] == \"yes\":\n",
    "        No_yes.append(\"No_yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_no = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and svm_prediction_list[i] == \"no\":\n",
    "        No_no.append(\"No_yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contengency Table between DNN and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=0.132, p-value=0.716\n",
      "Same proportions of errors (fail to reject H0)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# define contingency table\n",
    "table = [[185, 36],\n",
    "        [32, 194]]\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=False, correction=True)\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 yes\n",
      "1 yes\n",
      "2 yes\n",
      "3 yes\n",
      "4 yes\n",
      "5 yes\n",
      "6 yes\n",
      "7 yes\n",
      "8 yes\n",
      "9 yes\n",
      "10 yes\n",
      "11 yes\n",
      "12 yes\n",
      "13 yes\n",
      "14 yes\n",
      "15 yes\n",
      "16 yes\n",
      "17 yes\n",
      "18 yes\n",
      "19 yes\n",
      "20 yes\n",
      "21 yes\n",
      "22 no\n",
      "23 yes\n",
      "24 yes\n",
      "25 yes\n",
      "26 yes\n",
      "27 yes\n",
      "28 yes\n",
      "29 yes\n",
      "30 yes\n",
      "31 yes\n",
      "32 yes\n",
      "33 yes\n",
      "34 no\n",
      "35 yes\n",
      "36 yes\n",
      "37 yes\n",
      "38 yes\n",
      "39 yes\n",
      "40 yes\n",
      "41 yes\n",
      "42 yes\n",
      "43 yes\n",
      "44 yes\n",
      "45 yes\n",
      "46 yes\n",
      "47 yes\n",
      "48 yes\n",
      "49 yes\n",
      "50 yes\n",
      "51 yes\n",
      "52 yes\n",
      "53 yes\n",
      "54 yes\n",
      "55 yes\n",
      "56 yes\n",
      "57 yes\n",
      "58 yes\n",
      "59 yes\n",
      "60 yes\n",
      "61 yes\n",
      "62 yes\n",
      "63 yes\n",
      "64 no\n",
      "65 no\n",
      "66 yes\n",
      "67 yes\n",
      "68 yes\n",
      "69 yes\n",
      "70 yes\n",
      "71 yes\n",
      "72 yes\n",
      "73 yes\n",
      "74 yes\n",
      "75 no\n",
      "76 no\n",
      "77 no\n",
      "78 yes\n",
      "79 yes\n",
      "80 yes\n",
      "81 yes\n",
      "82 yes\n",
      "83 yes\n",
      "84 yes\n",
      "85 yes\n",
      "86 yes\n",
      "87 yes\n",
      "88 yes\n",
      "89 yes\n",
      "90 no\n",
      "91 yes\n",
      "92 yes\n",
      "93 yes\n",
      "94 yes\n",
      "95 yes\n",
      "96 yes\n",
      "97 yes\n",
      "98 yes\n",
      "99 yes\n",
      "100 yes\n",
      "101 yes\n",
      "102 yes\n",
      "103 yes\n",
      "104 yes\n",
      "105 yes\n",
      "106 yes\n",
      "107 yes\n",
      "108 yes\n",
      "109 yes\n",
      "110 yes\n",
      "111 yes\n",
      "112 yes\n",
      "113 yes\n",
      "114 yes\n",
      "115 yes\n",
      "116 yes\n",
      "117 yes\n",
      "118 yes\n",
      "119 no\n",
      "120 yes\n",
      "121 yes\n",
      "122 yes\n",
      "123 yes\n",
      "124 yes\n",
      "125 yes\n",
      "126 yes\n",
      "127 yes\n",
      "128 yes\n",
      "129 yes\n",
      "130 yes\n",
      "131 yes\n",
      "132 yes\n",
      "133 yes\n",
      "134 yes\n",
      "135 yes\n",
      "136 no\n",
      "137 no\n",
      "138 no\n",
      "139 yes\n",
      "140 yes\n",
      "141 yes\n",
      "142 yes\n",
      "143 yes\n",
      "144 yes\n",
      "145 yes\n",
      "146 yes\n",
      "147 yes\n",
      "148 yes\n",
      "149 yes\n",
      "150 yes\n",
      "151 yes\n",
      "152 yes\n",
      "153 yes\n",
      "154 yes\n",
      "155 yes\n",
      "156 yes\n",
      "157 yes\n",
      "158 no\n",
      "159 no\n",
      "160 yes\n",
      "161 yes\n",
      "162 yes\n",
      "163 yes\n",
      "164 yes\n",
      "165 yes\n",
      "166 yes\n",
      "167 yes\n",
      "168 yes\n",
      "169 yes\n",
      "170 yes\n",
      "171 yes\n",
      "172 yes\n",
      "173 yes\n",
      "174 yes\n",
      "175 yes\n",
      "176 yes\n",
      "177 yes\n",
      "178 yes\n",
      "179 yes\n",
      "180 yes\n",
      "181 yes\n",
      "182 yes\n",
      "183 yes\n",
      "184 yes\n",
      "185 yes\n",
      "186 yes\n",
      "187 yes\n",
      "188 yes\n",
      "189 yes\n",
      "190 yes\n",
      "191 yes\n",
      "192 yes\n",
      "193 yes\n",
      "194 yes\n",
      "195 yes\n",
      "196 yes\n",
      "197 yes\n",
      "198 yes\n",
      "199 yes\n",
      "200 yes\n",
      "201 yes\n",
      "202 yes\n",
      "203 yes\n",
      "204 yes\n",
      "205 yes\n",
      "206 yes\n",
      "207 yes\n",
      "208 yes\n",
      "209 yes\n",
      "210 yes\n",
      "211 yes\n",
      "212 yes\n",
      "213 yes\n",
      "214 yes\n",
      "215 yes\n",
      "216 yes\n",
      "217 yes\n",
      "218 yes\n",
      "219 yes\n",
      "220 no\n",
      "221 no\n",
      "222 yes\n",
      "223 yes\n",
      "224 yes\n",
      "225 yes\n",
      "226 yes\n",
      "227 yes\n",
      "228 yes\n",
      "229 yes\n",
      "230 no\n",
      "231 no\n",
      "232 yes\n",
      "233 yes\n",
      "234 yes\n",
      "235 yes\n",
      "236 yes\n",
      "237 yes\n",
      "238 yes\n",
      "239 no\n",
      "240 yes\n",
      "241 yes\n",
      "242 yes\n",
      "243 no\n",
      "244 yes\n",
      "245 yes\n",
      "246 yes\n",
      "247 yes\n",
      "248 no\n",
      "249 yes\n",
      "250 yes\n",
      "251 yes\n",
      "252 yes\n",
      "253 yes\n",
      "254 yes\n",
      "255 yes\n",
      "256 yes\n",
      "257 yes\n",
      "258 yes\n",
      "259 yes\n",
      "260 yes\n",
      "261 yes\n",
      "262 yes\n",
      "263 yes\n",
      "264 yes\n",
      "265 no\n",
      "266 no\n",
      "267 no\n",
      "268 yes\n",
      "269 yes\n",
      "270 yes\n",
      "271 no\n",
      "272 no\n",
      "273 yes\n",
      "274 yes\n",
      "275 yes\n",
      "276 yes\n",
      "277 yes\n",
      "278 no\n",
      "279 yes\n",
      "280 yes\n",
      "281 yes\n",
      "282 yes\n",
      "283 yes\n",
      "284 yes\n",
      "285 yes\n",
      "286 no\n",
      "287 no\n",
      "288 yes\n",
      "289 yes\n",
      "290 yes\n",
      "291 yes\n",
      "292 no\n",
      "293 yes\n",
      "294 yes\n",
      "295 yes\n",
      "296 yes\n",
      "297 yes\n",
      "298 yes\n",
      "299 yes\n",
      "300 yes\n",
      "301 yes\n",
      "302 yes\n",
      "303 yes\n",
      "304 yes\n",
      "305 yes\n",
      "306 yes\n",
      "307 yes\n",
      "308 yes\n",
      "309 yes\n",
      "310 yes\n",
      "311 yes\n",
      "312 yes\n",
      "313 yes\n",
      "314 yes\n",
      "315 yes\n",
      "316 yes\n",
      "317 yes\n",
      "318 yes\n",
      "319 no\n",
      "320 no\n",
      "321 yes\n",
      "322 no\n",
      "323 yes\n",
      "324 yes\n",
      "325 yes\n",
      "326 yes\n",
      "327 yes\n",
      "328 yes\n",
      "329 yes\n",
      "330 no\n",
      "331 no\n",
      "332 no\n",
      "333 no\n",
      "334 no\n",
      "335 no\n",
      "336 no\n",
      "337 yes\n",
      "338 yes\n",
      "339 no\n",
      "340 yes\n",
      "341 no\n",
      "342 no\n",
      "343 no\n",
      "344 yes\n",
      "345 no\n",
      "346 no\n",
      "347 yes\n",
      "348 yes\n",
      "349 yes\n",
      "350 yes\n",
      "351 yes\n",
      "352 yes\n",
      "353 yes\n",
      "354 yes\n",
      "355 yes\n",
      "356 yes\n",
      "357 yes\n",
      "358 yes\n",
      "359 yes\n",
      "360 yes\n",
      "361 yes\n",
      "362 no\n",
      "363 yes\n",
      "364 yes\n",
      "365 yes\n",
      "366 yes\n",
      "367 yes\n",
      "368 yes\n",
      "369 yes\n",
      "370 yes\n",
      "371 yes\n",
      "372 yes\n",
      "373 yes\n",
      "374 yes\n",
      "375 yes\n",
      "376 yes\n",
      "377 yes\n",
      "378 yes\n",
      "379 no\n",
      "380 no\n",
      "381 yes\n",
      "382 yes\n",
      "383 yes\n",
      "384 yes\n",
      "385 yes\n",
      "386 yes\n",
      "387 yes\n",
      "388 yes\n",
      "389 no\n",
      "390 no\n",
      "391 yes\n",
      "392 yes\n",
      "393 yes\n",
      "394 yes\n",
      "395 yes\n",
      "396 yes\n",
      "397 yes\n",
      "398 yes\n",
      "399 yes\n",
      "400 yes\n",
      "401 yes\n",
      "402 yes\n",
      "403 yes\n",
      "404 yes\n",
      "405 yes\n",
      "406 yes\n",
      "407 yes\n",
      "408 yes\n",
      "409 yes\n",
      "410 yes\n",
      "411 yes\n",
      "412 yes\n",
      "413 yes\n",
      "414 yes\n",
      "415 yes\n",
      "416 no\n",
      "417 yes\n",
      "418 yes\n",
      "419 yes\n",
      "420 yes\n",
      "421 yes\n",
      "422 yes\n",
      "423 yes\n",
      "424 yes\n",
      "425 yes\n",
      "426 no\n",
      "427 no\n",
      "428 no\n",
      "429 no\n",
      "430 no\n",
      "431 no\n",
      "432 no\n",
      "433 no\n",
      "434 yes\n",
      "435 yes\n",
      "436 yes\n",
      "437 yes\n",
      "438 yes\n",
      "439 yes\n",
      "440 no\n",
      "441 yes\n",
      "442 no\n",
      "443 yes\n",
      "444 yes\n",
      "445 yes\n",
      "446 yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:313: UserWarning: Trying to unpickle estimator GaussianNB from version 0.23.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# save the model to the disk \n",
    "filename_NB ='Gaussian_NB_model_nucleus_mitochondrion.sav'\n",
    "\n",
    "# some time later ...\n",
    "\n",
    "# load the model from disk \n",
    "model_NB = pickle.load(open(filename_NB,'rb'))\n",
    "\n",
    "y_pred = model_NB.predict(X_test)\n",
    "NB_list = []\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        print(i, \"yes\")\n",
    "\n",
    "        NB_list.append(\"yes\")\n",
    "    else:\n",
    "        print(i, \"no\")\n",
    "\n",
    "        NB_list.append(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_Yes = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and NB_list[i] == \"yes\":\n",
    "        Yes_Yes.append(\"Yes_Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_No = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and NB_list[i] == \"no\":\n",
    "        Yes_No.append(\"Yes_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_yes = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and NB_list[i] == \"yes\":\n",
    "        No_yes.append(\"No_yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_no = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and NB_list[i] == \"no\":\n",
    "        No_no.append(\"No_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=117.562, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# define contingency table\n",
    "table = [[190, 31],\n",
    "        [195, 31]]\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=False, correction=True)\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 yes\n",
      "1 yes\n",
      "2 yes\n",
      "3 yes\n",
      "4 yes\n",
      "5 yes\n",
      "6 yes\n",
      "7 yes\n",
      "8 yes\n",
      "9 no\n",
      "10 yes\n",
      "11 yes\n",
      "12 yes\n",
      "13 yes\n",
      "14 yes\n",
      "15 yes\n",
      "16 yes\n",
      "17 yes\n",
      "18 yes\n",
      "19 yes\n",
      "20 yes\n",
      "21 yes\n",
      "22 no\n",
      "23 yes\n",
      "24 yes\n",
      "25 yes\n",
      "26 yes\n",
      "27 yes\n",
      "28 yes\n",
      "29 yes\n",
      "30 yes\n",
      "31 yes\n",
      "32 yes\n",
      "33 yes\n",
      "34 no\n",
      "35 yes\n",
      "36 yes\n",
      "37 yes\n",
      "38 yes\n",
      "39 yes\n",
      "40 yes\n",
      "41 no\n",
      "42 yes\n",
      "43 yes\n",
      "44 yes\n",
      "45 yes\n",
      "46 yes\n",
      "47 yes\n",
      "48 yes\n",
      "49 no\n",
      "50 yes\n",
      "51 no\n",
      "52 no\n",
      "53 yes\n",
      "54 yes\n",
      "55 yes\n",
      "56 yes\n",
      "57 yes\n",
      "58 yes\n",
      "59 yes\n",
      "60 yes\n",
      "61 yes\n",
      "62 yes\n",
      "63 yes\n",
      "64 yes\n",
      "65 yes\n",
      "66 yes\n",
      "67 yes\n",
      "68 yes\n",
      "69 yes\n",
      "70 no\n",
      "71 yes\n",
      "72 yes\n",
      "73 yes\n",
      "74 yes\n",
      "75 no\n",
      "76 yes\n",
      "77 yes\n",
      "78 yes\n",
      "79 yes\n",
      "80 yes\n",
      "81 yes\n",
      "82 yes\n",
      "83 yes\n",
      "84 yes\n",
      "85 yes\n",
      "86 yes\n",
      "87 yes\n",
      "88 yes\n",
      "89 yes\n",
      "90 no\n",
      "91 yes\n",
      "92 yes\n",
      "93 yes\n",
      "94 yes\n",
      "95 yes\n",
      "96 yes\n",
      "97 yes\n",
      "98 yes\n",
      "99 yes\n",
      "100 yes\n",
      "101 yes\n",
      "102 yes\n",
      "103 yes\n",
      "104 yes\n",
      "105 yes\n",
      "106 yes\n",
      "107 no\n",
      "108 yes\n",
      "109 yes\n",
      "110 no\n",
      "111 yes\n",
      "112 yes\n",
      "113 yes\n",
      "114 yes\n",
      "115 yes\n",
      "116 yes\n",
      "117 no\n",
      "118 yes\n",
      "119 yes\n",
      "120 yes\n",
      "121 yes\n",
      "122 no\n",
      "123 yes\n",
      "124 yes\n",
      "125 yes\n",
      "126 yes\n",
      "127 yes\n",
      "128 no\n",
      "129 no\n",
      "130 no\n",
      "131 yes\n",
      "132 yes\n",
      "133 yes\n",
      "134 yes\n",
      "135 yes\n",
      "136 yes\n",
      "137 no\n",
      "138 yes\n",
      "139 yes\n",
      "140 yes\n",
      "141 yes\n",
      "142 yes\n",
      "143 yes\n",
      "144 yes\n",
      "145 yes\n",
      "146 yes\n",
      "147 yes\n",
      "148 yes\n",
      "149 yes\n",
      "150 yes\n",
      "151 yes\n",
      "152 yes\n",
      "153 yes\n",
      "154 yes\n",
      "155 yes\n",
      "156 yes\n",
      "157 yes\n",
      "158 no\n",
      "159 yes\n",
      "160 yes\n",
      "161 no\n",
      "162 yes\n",
      "163 yes\n",
      "164 yes\n",
      "165 yes\n",
      "166 yes\n",
      "167 yes\n",
      "168 yes\n",
      "169 no\n",
      "170 no\n",
      "171 yes\n",
      "172 no\n",
      "173 no\n",
      "174 no\n",
      "175 no\n",
      "176 no\n",
      "177 yes\n",
      "178 yes\n",
      "179 yes\n",
      "180 no\n",
      "181 yes\n",
      "182 no\n",
      "183 yes\n",
      "184 yes\n",
      "185 no\n",
      "186 yes\n",
      "187 no\n",
      "188 no\n",
      "189 yes\n",
      "190 yes\n",
      "191 no\n",
      "192 yes\n",
      "193 yes\n",
      "194 no\n",
      "195 yes\n",
      "196 yes\n",
      "197 yes\n",
      "198 yes\n",
      "199 yes\n",
      "200 no\n",
      "201 no\n",
      "202 yes\n",
      "203 yes\n",
      "204 yes\n",
      "205 yes\n",
      "206 yes\n",
      "207 no\n",
      "208 yes\n",
      "209 yes\n",
      "210 yes\n",
      "211 yes\n",
      "212 yes\n",
      "213 yes\n",
      "214 yes\n",
      "215 yes\n",
      "216 yes\n",
      "217 yes\n",
      "218 yes\n",
      "219 yes\n",
      "220 no\n",
      "221 no\n",
      "222 no\n",
      "223 no\n",
      "224 no\n",
      "225 no\n",
      "226 no\n",
      "227 yes\n",
      "228 no\n",
      "229 yes\n",
      "230 no\n",
      "231 no\n",
      "232 no\n",
      "233 no\n",
      "234 no\n",
      "235 yes\n",
      "236 yes\n",
      "237 yes\n",
      "238 yes\n",
      "239 no\n",
      "240 yes\n",
      "241 no\n",
      "242 yes\n",
      "243 no\n",
      "244 yes\n",
      "245 yes\n",
      "246 no\n",
      "247 no\n",
      "248 no\n",
      "249 yes\n",
      "250 yes\n",
      "251 no\n",
      "252 yes\n",
      "253 yes\n",
      "254 no\n",
      "255 no\n",
      "256 no\n",
      "257 no\n",
      "258 yes\n",
      "259 no\n",
      "260 yes\n",
      "261 yes\n",
      "262 no\n",
      "263 no\n",
      "264 no\n",
      "265 no\n",
      "266 no\n",
      "267 no\n",
      "268 no\n",
      "269 no\n",
      "270 yes\n",
      "271 no\n",
      "272 yes\n",
      "273 yes\n",
      "274 yes\n",
      "275 yes\n",
      "276 yes\n",
      "277 no\n",
      "278 no\n",
      "279 no\n",
      "280 no\n",
      "281 no\n",
      "282 no\n",
      "283 yes\n",
      "284 no\n",
      "285 no\n",
      "286 no\n",
      "287 no\n",
      "288 no\n",
      "289 no\n",
      "290 no\n",
      "291 no\n",
      "292 no\n",
      "293 yes\n",
      "294 no\n",
      "295 no\n",
      "296 no\n",
      "297 no\n",
      "298 yes\n",
      "299 yes\n",
      "300 yes\n",
      "301 yes\n",
      "302 no\n",
      "303 no\n",
      "304 no\n",
      "305 yes\n",
      "306 no\n",
      "307 no\n",
      "308 no\n",
      "309 yes\n",
      "310 no\n",
      "311 no\n",
      "312 no\n",
      "313 no\n",
      "314 yes\n",
      "315 yes\n",
      "316 no\n",
      "317 no\n",
      "318 no\n",
      "319 no\n",
      "320 no\n",
      "321 no\n",
      "322 no\n",
      "323 no\n",
      "324 yes\n",
      "325 yes\n",
      "326 no\n",
      "327 no\n",
      "328 yes\n",
      "329 no\n",
      "330 no\n",
      "331 yes\n",
      "332 no\n",
      "333 no\n",
      "334 no\n",
      "335 no\n",
      "336 no\n",
      "337 yes\n",
      "338 no\n",
      "339 no\n",
      "340 no\n",
      "341 no\n",
      "342 yes\n",
      "343 no\n",
      "344 yes\n",
      "345 yes\n",
      "346 yes\n",
      "347 no\n",
      "348 no\n",
      "349 no\n",
      "350 no\n",
      "351 no\n",
      "352 no\n",
      "353 no\n",
      "354 no\n",
      "355 yes\n",
      "356 no\n",
      "357 yes\n",
      "358 no\n",
      "359 no\n",
      "360 no\n",
      "361 no\n",
      "362 no\n",
      "363 no\n",
      "364 no\n",
      "365 no\n",
      "366 yes\n",
      "367 yes\n",
      "368 no\n",
      "369 no\n",
      "370 no\n",
      "371 no\n",
      "372 yes\n",
      "373 yes\n",
      "374 yes\n",
      "375 no\n",
      "376 yes\n",
      "377 no\n",
      "378 no\n",
      "379 no\n",
      "380 no\n",
      "381 yes\n",
      "382 yes\n",
      "383 yes\n",
      "384 yes\n",
      "385 yes\n",
      "386 yes\n",
      "387 no\n",
      "388 no\n",
      "389 no\n",
      "390 no\n",
      "391 no\n",
      "392 yes\n",
      "393 yes\n",
      "394 no\n",
      "395 yes\n",
      "396 no\n",
      "397 yes\n",
      "398 yes\n",
      "399 no\n",
      "400 no\n",
      "401 no\n",
      "402 yes\n",
      "403 no\n",
      "404 yes\n",
      "405 no\n",
      "406 no\n",
      "407 yes\n",
      "408 no\n",
      "409 yes\n",
      "410 no\n",
      "411 no\n",
      "412 yes\n",
      "413 yes\n",
      "414 yes\n",
      "415 no\n",
      "416 no\n",
      "417 no\n",
      "418 yes\n",
      "419 yes\n",
      "420 yes\n",
      "421 no\n",
      "422 no\n",
      "423 yes\n",
      "424 yes\n",
      "425 no\n",
      "426 yes\n",
      "427 yes\n",
      "428 yes\n",
      "429 no\n",
      "430 yes\n",
      "431 no\n",
      "432 yes\n",
      "433 no\n",
      "434 yes\n",
      "435 yes\n",
      "436 no\n",
      "437 yes\n",
      "438 yes\n",
      "439 no\n",
      "440 yes\n",
      "441 yes\n",
      "442 yes\n",
      "443 yes\n",
      "444 yes\n",
      "445 no\n",
      "446 no\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:313: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.23.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "C:\\Users\\philp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:313: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.23.1 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report,auc\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dense, Bidirectional,Dense, LSTM, Activation, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "import os \n",
    "\n",
    "\n",
    "# save the model to the disk \n",
    "filename_random_forest_123 ='Random_Forest_model_nucleus_mitochondrion.sav'\n",
    "\n",
    "# some time later ...\n",
    "rf_prediction = []\n",
    "# load the model from disk \n",
    "model_rf = pickle.load(open(filename_random_forest_123,'rb'))\n",
    "\n",
    "y_pred = model_rf.predict(X_test)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i]>0.5:\n",
    "        print(i, \"yes\")\n",
    "        rf_prediction.append(\"yes\")\n",
    "    else:\n",
    "        print(i, \"no\")\n",
    "        rf_prediction.append(\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rf_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "447"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DNN_prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_Yes = []\n",
    "for i in range(len(DNN_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and rf_prediction[i] == \"yes\":\n",
    "        Yes_Yes.append(\"Yes_Yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes_No = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"yes\" and rf_prediction[i] == \"no\":\n",
    "        Yes_No.append(\"Yes_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Yes_No)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_yes = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and rf_prediction[i] == \"yes\":\n",
    "        No_yes.append(\"No_yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_no = []\n",
    "for i in range(len(svm_prediction_list)):\n",
    "    if DNN_prediction_list[i] == \"no\" and rf_prediction[i] == \"no\":\n",
    "        No_no.append(\"No_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(No_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic=15.673, p-value=0.000\n",
      "Different proportions of errors (reject H0)\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# define contingency table\n",
    "table = [[172, 49],\n",
    "        [98, 128]]\n",
    "# calculate mcnemar test\n",
    "result = mcnemar(table, exact=False, correction=True)\n",
    "# summarize the finding\n",
    "print('statistic=%.3f, p-value=%.3f' % (result.statistic, result.pvalue))\n",
    "# interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue > alpha:\n",
    "    print('Same proportions of errors (fail to reject H0)')\n",
    "else:\n",
    "    print('Different proportions of errors (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\philp\\anaconda3\\lib\\site-packages\\sklearn\\base.py:193: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XGBClassifier' object has no attribute 'enable_categorical'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-689eb5951040>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mmodel_XGB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_XGB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_XGB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[0miteration_range\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1283\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m-> 1284\u001b[1;33m         class_probs = super().predict(\n\u001b[0m\u001b[0;32m   1285\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m    877\u001b[0m         )\n\u001b[0;32m    878\u001b[0m         \u001b[0miteration_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_iteration_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    880\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m                 predts = self.get_booster().inplace_predict(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_can_use_inplace_predict\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0mpredictor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"predictor\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         if (\n\u001b[1;32m--> 813\u001b[1;33m             \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"gblinear\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XGBClassifier' object has no attribute 'enable_categorical'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import sort\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report,auc\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import Dense, Bidirectional,Dense, LSTM, Activation, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "import os \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save the model to the disk \n",
    "filename_XGB ='xgb_model_nucleus_mitochondrion.sav'\n",
    "\n",
    "# some time later ...\n",
    "\n",
    "# load the model from disk \n",
    "model_XGB = pickle.load(open(filename_XGB,'rb'))\n",
    "\n",
    "y_pred = model_XGB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.0-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\philp\\anaconda3\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\philp\\anaconda3\\lib\\site-packages (from xgboost) (1.18.5)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\philp\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "0.6032 ANN model .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
